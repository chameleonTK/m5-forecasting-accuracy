# What we have done?
### TK
* Trained a model using all data at once but it was not that good. It trends to return just zeros. I think it might because there is a lot of zeros in the training data. I examined the label and found that it has the exponential-like distribution which means there are a lot of small values but even you removed all 0s, you still get a lot of 1s and so on. This will introduce so kind of bias?? (I think)

### Kenny
* Ploted the average sales (y-axis) and days(x-axis) to show that they has different distributions across different item category. So it is good to train models seperately on each category.

* How many years to use in the model?
   * TK: Just use 1-2 year(s) followed the notebook we have because he found that there is the memory limit if you run it in the Kaggle and also Google Colab. So it wont fit for more than this. But Kenny what to investigate further.
   
* Features? He suggests to introduce more features: rolling average/std with diffent windows and so on.
    * Please see `One Big LightGBM.ipynb` to get started. 

* He suggested a plan but TK thought it will be impractical.

### Jessica
* Userstand the resource we have so far

# What will we do next?
* Kenny will work according to his plan. Finalise how many data to train and features. Expect to see evaluation on the feature importance next week.
* TK will implement differnt kind of obejctive functions because RMSE is not that good to reflect the actual matric the compettition used.
* Jessica will see the expoential smoothing/ or some more features (TK suggests to read statistical approach so next week we can discuss more about it)
